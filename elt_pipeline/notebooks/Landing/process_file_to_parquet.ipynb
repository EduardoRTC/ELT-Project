{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================================\n",
    "#  Landing ➜ Raw/dia=YYYY‑MM‑DD/hora=HHMMSS ➜ 1 Parquet\n",
    "# ============================================================\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pathlib import PurePosixPath\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ───────────── 1. Configuração fixa ─────────────\n",
    "catalog = \"workspace\"\n",
    "schema  = \"default\"\n",
    "volume  = \"elt_volume\"\n",
    "\n",
    "vol_root = PurePosixPath(\"/Volumes\") / catalog / schema / volume\n",
    "landing  = vol_root / \"landing\"\n",
    "raw_root = vol_root / \"raw\"\n",
    "\n",
    "today_str = date.today().isoformat()                # 2025‑07‑29\n",
    "time_str  = datetime.now().strftime(\"%H%M%S\")       # e.g., 172637\n",
    "raw_part  = raw_root / f\"dia={today_str}\" / f\"hora={time_str}\"\n",
    "\n",
    "landing_csv = landing / \"vendas_mock.csv\"\n",
    "raw_csv     = raw_part / \"vendas_mock.csv\"\n",
    "parquet_file = raw_part / f\"vendas_mock_{time_str}.parquet\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ───────────── 2. Garante Volume e diretórios ─────────────\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\")\n",
    "for p in [landing, raw_part]:\n",
    "    dbutils.fs.mkdirs(str(p))\n",
    "\n",
    "# ───────────── 3. Valida arquivo em landing ─────────────\n",
    "if not dbutils.fs.ls(str(landing_csv)):\n",
    "    raise FileNotFoundError(f\"Arquivo não encontrado em {landing_csv}\")\n",
    "\n",
    "# ───────────── 4. Move (sobrescreve) para Raw ────────────\n",
    "try:\n",
    "    dbutils.fs.rm(str(raw_csv))\n",
    "except Exception:\n",
    "    pass\n",
    "dbutils.fs.mv(str(landing_csv), str(raw_csv))\n",
    "print(f\"✅ Movido para {raw_csv}\")\n",
    "\n",
    "# ───────────── 5. Lê CSV e grava Parquet único ──────────\n",
    "try:\n",
    "    # Verifica se o arquivo existe\n",
    "    if not dbutils.fs.ls(str(raw_csv)):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado em {raw_csv}\")\n",
    "\n",
    "    # Tenta ler o CSV com Spark (prioridade, já que Pandas falhou)\n",
    "    try:\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"dbfs:{raw_csv}\")\n",
    "        print(\"Schema do DataFrame Spark:\")\n",
    "        df.printSchema()\n",
    "        print(\"Conteúdo do CSV:\")\n",
    "        df.show(5)\n",
    "        \n",
    "        # Salva como Parquet único\n",
    "        tmp_dir = str(raw_part / \"_tmp_parquet\")\n",
    "        df.coalesce(1).write.mode(\"overwrite\").parquet(f\"dbfs:{tmp_dir}\")\n",
    "        \n",
    "        # Move part-file para nome final\n",
    "        part_file = [f.path for f in dbutils.fs.ls(f\"dbfs:{tmp_dir}\") if f.path.endswith(\".parquet\")][0]\n",
    "        # Remove destino se existir\n",
    "        try:\n",
    "            dbutils.fs.rm(f\"dbfs:{parquet_file}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        dbutils.fs.mv(part_file, f\"dbfs:{parquet_file}\")\n",
    "        dbutils.fs.rm(f\"dbfs:{tmp_dir}\", recurse=True)\n",
    "        \n",
    "        print(f\"✅ Parquet único salvo em {parquet_file}\")\n",
    "        \n",
    "        # Verifica o Parquet\n",
    "        parquet_df = spark.read.parquet(f\"dbfs:{parquet_file}\")\n",
    "        print(\"Conteúdo do Parquet:\")\n",
    "        parquet_df.show(5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao processar com Spark: {str(e)}\")\n",
    "        print(\"Tentando ler com Pandas como última tentativa...\")\n",
    "        \n",
    "        # Fallback: Lê CSV com Pandas\n",
    "        fuse_csv = f\"/dbfs{raw_csv}\"\n",
    "        pdf = pd.read_csv(fuse_csv, sep=\",\", encoding=\"utf-8\")\n",
    "        print(\"Tipos de dados do DataFrame Pandas:\", pdf.dtypes)\n",
    "        print(\"Primeiras linhas do CSV:\")\n",
    "        print(pdf.head())\n",
    "        \n",
    "        # Tenta salvar como Parquet com Pandas\n",
    "        try:\n",
    "            temp_parquet = str(raw_part / f\"temp_vendas_mock_{time_str}.parquet\")\n",
    "            pdf.to_parquet(temp_parquet, engine=\"pyarrow\", index=False)\n",
    "            \n",
    "            # Move o arquivo Parquet para o destino final\n",
    "            try:\n",
    "                dbutils.fs.rm(f\"dbfs:{parquet_file}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            dbutils.fs.mv(f\"file:{temp_parquet}\", f\"dbfs:{parquet_file}\")\n",
    "            print(f\"✅ Parquet único salvo em {parquet_file}\")\n",
    "            \n",
    "            # Verifica o Parquet\n",
    "            parquet_df = spark.read.parquet(f\"dbfs:{parquet_file}\")\n",
    "            print(\"Conteúdo do Parquet:\")\n",
    "            parquet_df.show(5)\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Erro ao salvar Parquet com Pandas: {str(e2)}\")\n",
    "            print(\"🚨 Não foi possível processar o arquivo. Verifique o formato do CSV e as permissões no Volume.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao processar o arquivo: {str(e)}\")\n",
    "    print(\"🚨 Não foi possível processar o arquivo. Verifique o formato do CSV e as permissões no Volume.\")\n",
    "\n",
    "print(\"🏁 Processo concluído.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
