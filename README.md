# ELTâ€¯â€“â€¯AnÃ¡lises de performance e detecÃ§Ã£o de anomalias.
Arquitetura Medallion â€¢ Airflowâ€¯3â€¯â€¢â€¯PySparkâ€¯4â€¯â€¢â€¯Databricks

## 1. VisÃ£o Geral ğŸš€
Pipeline ELT em quatro camadas (Landingâ€¯â†’â€¯Bronzeâ€¯â†’â€¯Silverâ€¯â†’â€¯Gold) que:

* recebe arquivo `rel_vendas.xlsx`;
* limpa e enriquece os dados segundo regras de negÃ³cio;
* gera um **dataset Gold** pronto para anÃ¡lise de performance e detecÃ§Ã£o de anomalias.

### Tecnologias
* Apache **Airflowâ€¯3.0.3** (Pythonâ€¯3.12) â€“ orquestraÃ§Ã£o  
* **PySparkâ€¯4.0.0** â€“ transformaÃ§Ãµes distribuÃ­das  
* **Databricks** providerÂ 7.6.0 â€“ execuÃ§Ã£o em cluster / DBFS  
* **PostgreSQLâ€¯13** â€“ metadados do Airflow  
* DockerÂ +Â Compose â€“ empacotamento local

---

## 2. Estrutura do Arquivo de Origem
Arquivo entregue: `rel_vendas.csv`  

Principais campos (landing):

* `venda_id` â€“ id Ãºnico da venda  
* `data` â€“ data da venda  
* `vendedor_id`, `vendedor_nome`  
* `produto_id`, `produto_nome`, `categoria`  
* `preco_unit`, `qtd`, `desconto`  
* `canal` â€“ loja_fisica, ecommerce, app  
* `cidade`, `estado`, `comentarios`

---

## 3. Regras por Camada

### 3.1 Landing (Raw Zone)
* Apenas salva o arquivo original em `/landing/yyyy/MM/dd/`.  
* Registra nome e data de recebimento.

### 3.2 Bronze (Clean Zone)
* Remove linhas com nulos crÃ­ticos (`venda_id`, `data`, `produto_id`, `vendedor_id`, `preco_unit`, `qtd`).  
* Converte tipos (`data` â†’ date, `preco_unit` â†’ float, `qtd` â†’ int).  
* Padroniza nomes de colunas (snake_case).  

### 3.3 Silver (Trusted Zone)
* **`valor_total`** = `preco_unit * qtd * (1 - desconto)`.  
* **`score_venda`**  
  * ALTO_RISCO â†’ desconto >â€¯20â€¯%  
  * VENDA_PREMIUM â†’ valor_total >â€¯R$â€¯1â€¯000  
  * NORMAL â†’ demais casos  
* **`flag_anomalia`** â€“ TRUE se `preco_unit` estiver Â±30â€¯% fora da mÃ©dia da categoria.  
* Remove colunas de texto livre e endereÃ§o (`vendedor_nome`, `produto_nome`, `comentarios`, `cidade`, `estado`).  
* MantÃ©m apenas: ids, data, categoria, preÃ§o, quantidade, desconto, canal, valor_total, score_venda, flag_anomalia.

### 3.4 Gold (Final Zone)
* Exporta `vendas_gold_YYYYMMDD.csv` para `/gold/yyyy/MM/dd/`.  
* Campos finais: `venda_id`, `data`, `vendedor_id`, `produto_id`, `categoria`, `valor_total`, `desconto`, `score_venda`, `flag_anomalia`.

---

## 4. Controle de ExecuÃ§Ã£o
A cada execuÃ§Ã£o o Airflow grava:

* nome do arquivo e timestamp  
* camada processada  
* linhas recebidas / vÃ¡lidas  
* colunas removidas / mantidas  
* regras aplicadas  
* hash SHAâ€‘256 (opcional)  
* status (sucesso / erro)

---

## 5. Rodando Localmente ğŸƒâ€â™‚ï¸

### PrÃ©â€‘requisitos
* Dockerâ€¯20.10+ e Dockerâ€¯Composeâ€¯v2  
* Workspace e token Databricks

### Passo a passo

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/<user>/elt-vendas.git
cd elt-vendas

# 2. Crie pastas locais
mkdir -p dags logs plugins

# 3. Exporte variÃ¡veis do Databricks
export DATABRICKS_HOST="https://<workspace>.cloud.databricks.com"
export DATABRICKS_TOKEN="dapi<token>"

# 4. Suba tudo
docker compose up --build

O terminal exibirÃ¡ algo como:
Simple auth manager | Password for user 'admin': AbC123xYz